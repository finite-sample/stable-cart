{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Getting Started with Stable-CART Methods\n\nThis notebook demonstrates how to use stable-cart methods to analyze prediction stability using bootstrap variance measurement. We compare different stable tree implementations against a standard CART baseline.\n\n## Why Stability Matters\nStandard decision trees are **notoriously unstable** - small changes in training data can lead to completely different trees and predictions. Stable-CART methods modify the tree-building process to create trees that are **more consistent** across different training runs, which is crucial for:\n- **Model reliability** in production\n- **Interpretability** and trust  \n- **Reduced overfitting** to specific training samples"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score, roc_auc_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from stable_cart import LessGreedyHybridTree, BootstrapVariancePenalizedTree, RobustPrefixHonestTree\n",
    "\n",
    "plt.style.use('seaborn-v0_8')\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"üéØ Stable-CART Stability Analysis\")\n",
    "print(\"Comparing stable tree methods vs standard CART\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset: Digits Binary Classification\n",
    "\n",
    "We use the digits dataset with binary classification (digit 0 vs all others) because:\n",
    "- **High dimensionality** (64 features) where stability can be important\n",
    "- **Real-world data** with natural complexity\n",
    "- **Clear class separation** allowing measurement of stability effects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and prepare data\n",
    "data = load_digits()\n",
    "X, y = data.data, (data.target == 0).astype(int)\n",
    "\n",
    "# Standardize features for stable-cart methods\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Dataset characteristics:\")\n",
    "print(f\"  Features: {X.shape[1]}\")\n",
    "print(f\"  Training samples: {X_train.shape[0]}\")\n",
    "print(f\"  Test samples: {X_test.shape[0]}\")\n",
    "print(f\"  Class distribution: {dict(zip(*np.unique(y_test, return_counts=True)))}\")\n",
    "print(f\"  Task: Binary classification (digit 0 vs others)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stability Measurement Methodology\n",
    "\n",
    "We measure **prediction stability** using bootstrap variance:\n",
    "1. Train the same model on multiple bootstrap samples of the training data\n",
    "2. Make predictions on the same test set with each trained model\n",
    "3. Calculate the variance of predictions for each test point\n",
    "4. Lower variance = more stable predictions across training runs\n",
    "\n",
    "This simulates the real-world scenario where slight differences in training data (new samples, missing values, etc.) can lead to different model behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_prediction_stability(model_class, X_train, y_train, X_test, y_test, model_params, n_bootstrap=15):\n",
    "    \"\"\"\n",
    "    Measure prediction stability via bootstrap variance.\n",
    "    \n",
    "    Returns:\n",
    "        dict with variance statistics and performance metrics\n",
    "    \"\"\"\n",
    "    n_test = X_test.shape[0]\n",
    "    predictions = np.zeros((n_bootstrap, n_test))\n",
    "    aucs = []\n",
    "    \n",
    "    for i in range(n_bootstrap):\n",
    "        # Bootstrap sample\n",
    "        n_samples = X_train.shape[0]\n",
    "        bootstrap_idx = np.random.choice(n_samples, n_samples, replace=True)\n",
    "        X_boot = X_train[bootstrap_idx]\n",
    "        y_boot = y_train[bootstrap_idx]\n",
    "        \n",
    "        # Train model\n",
    "        params_copy = model_params.copy()\n",
    "        params_copy.pop('random_state', None)  # Avoid conflicts\n",
    "        model = model_class(**params_copy, random_state=i)\n",
    "        model.fit(X_boot, y_boot)\n",
    "        \n",
    "        # Predictions\n",
    "        y_pred = model.predict(X_test)\n",
    "        predictions[i] = y_pred\n",
    "        \n",
    "        # Performance\n",
    "        if hasattr(model, 'predict_proba'):\n",
    "            y_proba = model.predict_proba(X_test)[:, 1]\n",
    "            aucs.append(roc_auc_score(y_test, y_proba))\n",
    "    \n",
    "    # Calculate stability metrics\n",
    "    point_variances = np.var(predictions, axis=0)\n",
    "    \n",
    "    return {\n",
    "        'mean_variance': np.mean(point_variances),\n",
    "        'median_variance': np.median(point_variances),\n",
    "        'max_variance': np.max(point_variances),\n",
    "        'point_variances': point_variances,\n",
    "        'predictions': predictions,\n",
    "        'aucs': aucs,\n",
    "        'auc_mean': np.mean(aucs),\n",
    "        'auc_std': np.std(aucs)\n",
    "    }\n",
    "\n",
    "print(\"‚úÖ Stability measurement function ready\")\n",
    "print(\"üìä Will use 15 bootstrap samples per model for analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Configurations\n",
    "\n",
    "We test different stable-cart methods against a standard CART baseline:\n",
    "- **CART Baseline**: Standard decision tree for comparison\n",
    "- **LessGreedyHybrid**: Uses data partitioning for more cautious splitting\n",
    "- **RobustPrefixHonest**: Uses consensus and honest estimation for stability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model configurations for comparison\n",
    "model_configs = {\n",
    "    'CART_Baseline': {\n",
    "        'class': DecisionTreeClassifier,\n",
    "        'params': {\n",
    "            'max_depth': 10,\n",
    "            'min_samples_leaf': 1,\n",
    "            'random_state': 42\n",
    "        },\n",
    "        'description': 'Standard decision tree baseline'\n",
    "    },\n",
    "    \n",
    "    'LessGreedy_Method': {\n",
    "        'class': LessGreedyHybridTree,\n",
    "        'params': {\n",
    "            'task': 'classification',\n",
    "            'max_depth': 12,\n",
    "            'min_samples_leaf': 2,\n",
    "            'split_frac': 0.95,     # 95% data for splitting\n",
    "            'val_frac': 0.03,       # 3% for validation\n",
    "            'est_frac': 0.02,       # 2% for estimation\n",
    "            'enable_oblique_splits': False,\n",
    "            'enable_lookahead': False,\n",
    "            'random_state': 42\n",
    "        },\n",
    "        'description': 'Less greedy approach with data partitioning'\n",
    "    },\n",
    "    \n",
    "    'RobustPrefix_Method': {\n",
    "        'class': RobustPrefixHonestTree,\n",
    "        'params': {\n",
    "            'task': 'classification',\n",
    "            'max_depth': 10,\n",
    "            'min_samples_leaf': 2,\n",
    "            'top_levels': 2,          # Robust prefix levels\n",
    "            'consensus_samples': 3,   # Consensus mechanism\n",
    "            'val_frac': 0.05,\n",
    "            'est_frac': 0.03,\n",
    "            'random_state': 42\n",
    "        },\n",
    "        'description': 'Robust prefix with consensus and honest estimation'\n",
    "    }\n",
    "}\n",
    "\n",
    "print(f\"üìã Configured {len(model_configs)} models for comparison:\")\n",
    "for name, config in model_configs.items():\n",
    "    print(f\"   {name}: {config['description']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the Stability Analysis\n",
    "\n",
    "We now run the stability analysis, measuring both:\n",
    "- **Single-run performance** (accuracy, AUC)\n",
    "- **Bootstrap stability** (variance across training runs)\n",
    "\n",
    "‚è±Ô∏è *This may take a few minutes as we train multiple models*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üîÑ Running stability analysis...\")\n",
    "print(\"This trains 15 bootstrap models per configuration\")\n",
    "\n",
    "results = {}\n",
    "\n",
    "for name, config in model_configs.items():\n",
    "    print(f\"\\nüìä Analyzing {name}...\")\n",
    "    \n",
    "    # Single run performance\n",
    "    model = config['class'](**config['params'])\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Get performance metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    balanced_acc = balanced_accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    if hasattr(model, 'predict_proba'):\n",
    "        y_proba = model.predict_proba(X_test)[:, 1]\n",
    "        auc = roc_auc_score(y_test, y_proba)\n",
    "    else:\n",
    "        auc = balanced_acc\n",
    "    \n",
    "    unique_preds = len(np.unique(y_pred))\n",
    "    \n",
    "    print(f\"   Single run: AUC={auc:.3f}, Accuracy={accuracy:.3f}, Unique predictions={unique_preds}\")\n",
    "    \n",
    "    if unique_preds > 1 and balanced_acc > 0.8:  # Working model\n",
    "        # Measure stability\n",
    "        print(f\"   Measuring stability (15 bootstrap samples)...\")\n",
    "        stability = measure_prediction_stability(\n",
    "            config['class'], X_train, y_train, X_test, y_test, config['params']\n",
    "        )\n",
    "        \n",
    "        results[name] = {\n",
    "            'auc': auc,\n",
    "            'accuracy': accuracy,\n",
    "            'balanced_acc': balanced_acc,\n",
    "            'stability': stability,\n",
    "            'description': config['description'],\n",
    "            'working': True\n",
    "        }\n",
    "        \n",
    "        print(f\"   ‚úÖ Variance: {stability['mean_variance']:.4f}, AUC std: ¬±{stability['auc_std']:.3f}\")\n",
    "    else:\n",
    "        print(f\"   ‚ùå Model not working properly\")\n",
    "        results[name] = {\n",
    "            'working': False,\n",
    "            'description': config['description']\n",
    "        }\n",
    "\n",
    "print(f\"\\n‚úÖ Analysis complete! Found {sum(1 for r in results.values() if r.get('working', False))} working models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results: Stability vs Accuracy Analysis\n",
    "\n",
    "Let's analyze the trade-offs between prediction stability and accuracy performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter working models\n",
    "working_results = {name: res for name, res in results.items() if res.get('working', False)}\n",
    "\n",
    "if len(working_results) < 2:\n",
    "    print(\"‚ùå Need at least 2 working models for comparison\")\n",
    "else:\n",
    "    print(f\"üèÜ STABILITY vs ACCURACY ANALYSIS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Baseline metrics\n",
    "    baseline = working_results['CART_Baseline']\n",
    "    baseline_auc = baseline['auc']\n",
    "    baseline_variance = baseline['stability']['mean_variance']\n",
    "    \n",
    "    print(f\"\\nüìä BASELINE (CART):\")\n",
    "    print(f\"   AUC: {baseline_auc:.3f}\")\n",
    "    print(f\"   Prediction Variance: {baseline_variance:.4f}\")\n",
    "    \n",
    "    # Stable model comparison\n",
    "    stable_models = {name: res for name, res in working_results.items() if name != 'CART_Baseline'}\n",
    "    \n",
    "    print(f\"\\nüîÑ STABLE-CART MODELS:\")\n",
    "    \n",
    "    comparison_data = []\n",
    "    \n",
    "    for name, res in stable_models.items():\n",
    "        variance = res['stability']['mean_variance']\n",
    "        auc = res['auc']\n",
    "        \n",
    "        # Calculate improvements\n",
    "        variance_improvement = (1 - variance / baseline_variance) * 100\n",
    "        performance_ratio = (auc / baseline_auc) * 100\n",
    "        \n",
    "        comparison_data.append({\n",
    "            'Model': name.replace('_', ' '),\n",
    "            'AUC': auc,\n",
    "            'Variance': variance,\n",
    "            'Variance_Improvement': variance_improvement,\n",
    "            'Performance_Ratio': performance_ratio,\n",
    "            'Description': res['description']\n",
    "        })\n",
    "        \n",
    "        print(f\"\\n   {name}:\")\n",
    "        print(f\"      AUC: {auc:.3f} ({performance_ratio:.1f}% of baseline)\")\n",
    "        print(f\"      Variance: {variance:.4f} ({variance_improvement:+.1f}% vs baseline)\")\n",
    "        print(f\"      {res['description']}\")\n",
    "        \n",
    "        if variance_improvement > 25:\n",
    "            print(f\"      üéâ OUTSTANDING variance reduction!\")\n",
    "        elif variance_improvement > 15:\n",
    "            print(f\"      üèÜ EXCELLENT variance reduction!\")\n",
    "        elif variance_improvement > 5:\n",
    "            print(f\"      ‚úÖ GOOD variance reduction\")\n",
    "        else:\n",
    "            print(f\"      ‚ö†Ô∏è Marginal improvement\")\n",
    "    \n",
    "    # Create comparison DataFrame\n",
    "    df_comparison = pd.DataFrame(comparison_data)\n",
    "    df_comparison = df_comparison.sort_values('Variance_Improvement', ascending=False)\n",
    "    \n",
    "    print(f\"\\nüìä SUMMARY TABLE:\")\n",
    "    print(df_comparison[['Model', 'AUC', 'Variance_Improvement', 'Performance_Ratio']].to_string(index=False, float_format='%.1f'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization: Stability vs Accuracy Trade-off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualizations\n",
    "if len(working_results) >= 2:\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    \n",
    "    colors = ['#2E86AB', '#A23B72', '#F18F01', '#C73E1D', '#92AA83']\n",
    "    \n",
    "    # 1. Stability vs Accuracy scatter plot\n",
    "    ax1 = axes[0]\n",
    "    \n",
    "    for i, (name, res) in enumerate(working_results.items()):\n",
    "        stability_score = 1 / (1 + res['stability']['mean_variance'])\n",
    "        auc = res['auc']\n",
    "        \n",
    "        ax1.scatter(stability_score, auc, s=300, color=colors[i % len(colors)], \n",
    "                   alpha=0.8, edgecolors='black', linewidth=2, label=name.replace('_', ' '))\n",
    "    \n",
    "    ax1.set_xlabel('Stability Score (Higher = More Stable)', fontsize=12)\n",
    "    ax1.set_ylabel('AUC Score (Higher = Better)', fontsize=12)\n",
    "    ax1.set_title('Stability vs Accuracy Trade-off', fontsize=14, fontweight='bold')\n",
    "    ax1.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. Variance comparison bar chart\n",
    "    ax2 = axes[1]\n",
    "    names = [name.replace('_', '\\n') for name in working_results.keys()]\n",
    "    variances = [res['stability']['mean_variance'] for res in working_results.values()]\n",
    "    \n",
    "    bars = ax2.bar(range(len(names)), variances, color=colors[:len(names)], alpha=0.8, edgecolor='black')\n",
    "    ax2.set_ylabel('Prediction Variance (Lower = Better)', fontsize=12)\n",
    "    ax2.set_title('Model Prediction Variance', fontsize=14, fontweight='bold')\n",
    "    ax2.set_xticks(range(len(names)))\n",
    "    ax2.set_xticklabels(names, rotation=45, ha='right')\n",
    "    \n",
    "    # Add values on bars\n",
    "    for bar, var in zip(bars, variances):\n",
    "        ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(variances)*0.01,\n",
    "                f'{var:.4f}', ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "    \n",
    "    # 3. Performance comparison\n",
    "    ax3 = axes[2]\n",
    "    aucs = [res['auc'] for res in working_results.values()]\n",
    "    \n",
    "    bars = ax3.bar(range(len(names)), aucs, color=colors[:len(names)], alpha=0.8, edgecolor='black')\n",
    "    ax3.set_ylabel('AUC Score (Higher = Better)', fontsize=12)\n",
    "    ax3.set_title('Model Performance Comparison', fontsize=14, fontweight='bold')\n",
    "    ax3.set_xticks(range(len(names)))\n",
    "    ax3.set_xticklabels(names, rotation=45, ha='right')\n",
    "    ax3.set_ylim(min(aucs) * 0.98, max(aucs) * 1.02)\n",
    "    \n",
    "    # Add values on bars\n",
    "    for bar, auc in zip(bars, aucs):\n",
    "        ax3.text(bar.get_x() + bar.get_width()/2, bar.get_height() + (max(aucs) - min(aucs))*0.01,\n",
    "                f'{auc:.3f}', ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "    \n",
    "    plt.suptitle('Stable-CART: Stability vs Accuracy Analysis', \n",
    "                 fontsize=16, fontweight='bold', y=1.05)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}